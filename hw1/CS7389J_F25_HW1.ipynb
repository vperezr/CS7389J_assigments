{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SgNZTjrhcHa0"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e5d934fabac347239909c94b7e9ab80d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c80a3e7f234349f59a6eb6d01ffd87d1",
              "IPY_MODEL_36cbd9113b814b6a97dde6c38138f513",
              "IPY_MODEL_7ccf22b67fd64d9490e7947f8a2cf1f8"
            ],
            "layout": "IPY_MODEL_6cb7e5a14b8e42d9a36465c58fbe068b"
          }
        },
        "c80a3e7f234349f59a6eb6d01ffd87d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c4d31961c964efdb118548bd7a9bac3",
            "placeholder": "​",
            "style": "IPY_MODEL_50389c471d0f4ec583e6afae496c6536",
            "value": "model.safetensors: 100%"
          }
        },
        "36cbd9113b814b6a97dde6c38138f513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb7b003002c740f1809308ce3eeff41d",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c7f6d4ffea14f2e98dd349564f7893c",
            "value": 440449768
          }
        },
        "7ccf22b67fd64d9490e7947f8a2cf1f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfa98f04627d404cbcbe6634e2c74bac",
            "placeholder": "​",
            "style": "IPY_MODEL_d374d8e22b9b4ef7b39970451d804520",
            "value": " 440M/440M [00:05&lt;00:00, 78.8MB/s]"
          }
        },
        "6cb7e5a14b8e42d9a36465c58fbe068b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c4d31961c964efdb118548bd7a9bac3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50389c471d0f4ec583e6afae496c6536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb7b003002c740f1809308ce3eeff41d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c7f6d4ffea14f2e98dd349564f7893c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfa98f04627d404cbcbe6634e2c74bac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d374d8e22b9b4ef7b39970451d804520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgNZTjrhcHa0"
      },
      "source": [
        "## Homework 1, CS685 Spring 2024\n",
        "\n",
        "### This assigmentis due on TODO, 2025. This notebook is to be submitted via Gradescope as a PDF file, along with your three dataset files (annotator1.csv, annotator2.csv, and final_data.csv).\n",
        "\n",
        "#### IMPORTANT: After copying this notebook to your Google Drive, please paste a link to it below. To get a publicly-accessible link, hit the *Share* button at the top right, then click \"Get shareable link\" and copy over the result. If you fail to do this, you will receive no credit for this homework!\n",
        "\n",
        "***YOUR LINK HERE:***\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##### *How to submit this assigment:*\n",
        "- Write all your answers in this Colab notebook. Once you are finished, generate a PDF via (File -> Print -> Save as PDF) and upload it to HW1 in Canvas.\n",
        "  \n",
        "- **Important:** check your PDF before you submit to Gradescope to make sure it exported correctly. If Colab gets confused about your syntax, it will sometimes terminate the PDF creation routine early.\n",
        "\n",
        "- **Important:** on Gradescope, please make sure that you label each page with the corresponding question(s). This makes it significantly easier for our graders to grade submissions, especially with the long outputs of many of these cells. We will take off points for submissions that are not properly labeled.\n",
        "\n",
        "- When creating your final version of the PDF to hand in, please do a fresh restart and execute every cell in order. One handy way to do this is by clicking `Runtime -> Run All` in the notebook menu.\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a serious case of cheating. See the course page for student honesty policies.\n",
        "\n",
        "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "- There is no penalty for using AI assistance on this homework as long as you fully disclose it in the final cell of this notebook (this includes storing any prompts that you feed to large language models). That said, anyone caught using AI assistance without proper disclosure will receive a zero on the assignment (we have several automatic tools to detect such cases). We're literally allowing you to use it with no limitations, so there is no reason to lie!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2dYC4HbZL-j"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Part 1: Annotation\n",
        "\n",
        "In this homework, you will first collect a labeled dataset of **200** sentences for a text classification task of your choice. This process will include:\n",
        "\n",
        "1. *Data collection*: Collect **200** sentences from any source you find interesting (e.g., literature, social media posts, news articles, reviews, etc.)\n",
        "\n",
        "2. *Task design*: Design a binary (i.e., only two labels) sentence-level classification task that you would like to perform on your sentences. Be creative, and make sure your task is not too easy (e.g., the label assigment have some degree of subjectivity to them, or human knowledge/intuition is required to correctly annotate an example). Write detailed annotator guidelines/instructions on how you would like people to label your data. Provide examples when relevant and make sure people is able to follow the guidelines, e.g., ask for peer feedback about the clarity of your guidelines.  \n",
        "\n",
        "3. On your dataset, collect annotations from **two** classmates for your task. Everyone in this class will need to both create their own dataset and also serve as an annotator for two other classmates. In order to get everything done on time, you need to complete the following steps:\n",
        "\n",
        "> *   Find two classmates willing to label 120 sentences each. Star this early as without data you will not be able to complete the assigment on time.\n",
        "*   Send them your annotation guidelines and a way that they can easily annotate the data (e.g., a spreadsheet or Google form).\n",
        "*   Collect the labeled data from each of the two annotators.\n",
        "*   Sanity check the data for basic cleanliness (are all examples annotated? are all labels allowable ones?)\n",
        "\n",
        "4. Collect feedback from annotators about the task including annotation time and obstacles encountered (e.g., maybe your guidelines were confusing! or maybe some sentences were particularly hard to annotate!).\n",
        "\n",
        "5. Calculate and report inter-annotator agreement.\n",
        "\n",
        "6. Aggregate output from both annotators to create final dataset.\n",
        "\n",
        "7. Perform NLP experiments on your new dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Heui1z3IjZh_"
      },
      "source": [
        "## Question 1.1 (10 points):\n",
        "Describe the source of your unlabeled data, why you chose it, and what kind of sentence selection process you used (if any) to choose 200 sentences for annotation. Also briefly describe the text classification task that will be conducting using this data.\n",
        "\n",
        "### *WRITE YOUR ANSWER HERE* ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EynpTLydj7IM"
      },
      "source": [
        "## Question 1.2 (25 points):\n",
        "Copy the annotation guidelines that you provided to your annotators below. We expect that these guidelines will be very detailed (and as such could be fairly long). You must include:\n",
        "\n",
        "*   The two categories for your binary classification problem, including the exact strings you want annotators to use while labeling.\n",
        "*   Descriptions of the categories and what they mean.\n",
        "*   Representative examples of each category (i.e., sentences from outside your dataset that you have manually labeled to give annotators an idea of how to perform the task)\n",
        "*   A discussion of tricky corner cases, and criteria to help the annotator decide them. If you look at the data and think about how an annotator could do the task, you will likely find a bunch of these!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### *COPY YOUR ANNOTATION GUIDELINES HERE.* Please format them nicely so it i seasy to read / grade them  ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "935duWappc-o"
      },
      "source": [
        "## Question 1.3 (5 points):\n",
        "Write down the names and emails of the two classmates who will be annotating your data below. Once they are finished annotating, create two .csv files (annotator1.csv and annotator2.csv) that contains each annotator's labels. The file should have two columns with headers **text** and **label**, respectively. You will include these in your assigment submission.\n",
        "\n",
        "*The tweets.csv file provided as an example in Part 2 below uses the same format.*\n",
        "\n",
        "---\n",
        "### *WRITE CLASSMATE 1 NAME/EMAIL HERE:* ###\n",
        "### *WRITE CLASSMATE 2 NAME/EMAIL HERE:* ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcIMegO_uPRK"
      },
      "source": [
        "## Question 1.4 (10 points):\n",
        "After both annotators have finished labeling the 120 sentences you gave them, ask them for feedback about your task and the provided annotation guidelines. If you were to collect more labeled data for this task in the future, what would you change from your current setup? Why? Please include a summary of annotator feedback (with specific examples that they found challenging to label) in your answer.\n",
        "\n",
        "### *WRITE ANSWER HERE* ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ltte0Z-vD0u"
      },
      "source": [
        "## Question 1.5 (10 points):\n",
        "Now, compute the inter-annotator agreement between your two annotators. Upload both .csv files to your Colab session (click the folder icon in the sidebar to the left of the screen). In the code cell below, read the data from the two files and compute both the raw agreement (% of examples for which both annotators agreed on the label) and the [Cohen's Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa). Feel free to use implementations in existing libraries (e.g., [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html)). After you're done, paste the numbers in the text cell that follows your code.\n",
        "\n",
        "*A note on inter-annotator score interpretation: Cohen suggested the Kappa result be interpreted as follows: values ≤ 0 as indicating no agreement and 0.01–0.20 as none to slight, 0.21–0.40 as fair, 0.41– 0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1.00 as almost perfect agreement. Overall, we seek to have an above than average agreement to be able to use our annotations as training data, but this can change depending on the complexity of the task*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD7yvkwgy82S"
      },
      "source": [
        "### WRITE CODE TO LOAD ANNOTATIONS AND\n",
        "### COMPUTE AGREEMENT + COHEN'S KAPPA HERE!"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd7Xq4SKzF5U"
      },
      "source": [
        "###*RAW AGREEMENT*:\n",
        "###*COHEN'S KAPPA*:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2-1LkRHze5N"
      },
      "source": [
        "## Question 1.6 (10 points):\n",
        "To form your final dataset, you need to *aggregate* the annotations from both annotators (i.e., for cases where they disagree, you need to choose a single label). Use any method you like other than random label selection to perform this aggregation (e.g., have the two annotators discuss each disagreement and come to consensus, or choose the label you agree with the most i.e., act as a tiebreaker). Upload your final dataset to the Colab session (in the same format as the other two files) as final_dataset.csv. Remember to include this file in your submission!\n",
        "\n",
        "---\n",
        "\n",
        "### *DESCRIBE YOUR AGGREGATION STRATEGY HERE* ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d23zfO_ALKeB"
      },
      "source": [
        "# Part 2: Text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKc0xYh-MAbc"
      },
      "source": [
        "##Data Prep and Model Specifications\n",
        "\n",
        "Upload your data using the file explorer to the left. We have provided a function below to tokenize and format your data as BERT requires. Make sure that your csv file, titled final_data.csv, has one column \"text\" and another column \"labels\" containing integers.\n",
        "\n",
        "If you run the cell below without modifications, it will run on the tweets.csv example data we have provided. It imports some helper functions we wrote to demonstrate the task on the sample tweet dataset. You should first run all of the following cells with tweets.csv just to see how everything works. Then, once you understand the whole preprocessing / fine-tuning process, change the csv in the below cell to your final_data.csv file, add any extra preprocessing code you wish, and then run the cells again on your own data.\n",
        "\n",
        "We have provided a sample data file called tweets.csv that contains tweets about airlines along with a negative, neutral, or positive sentiment rating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGgeZ3M0UWs0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c157e7-11e5-43d7-ae41-9e9ce8e595a1"
      },
      "source": [
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vperezr/CS7389J_assigments/main/hw1/tweets.csv\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Load and shuffle data\n",
        "df = pd.read_csv(\"tweets.csv\")\n",
        "# Load data your own data\n",
        "# df = pd.read_csv(\"final_data.csv\")\n",
        "df = shuffle(df, random_state=42).reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-11 21:27:27--  https://raw.githubusercontent.com/vperezr/CS7389J_assigments/main/hw1/tweets.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21512 (21K) [text/plain]\n",
            "Saving to: ‘tweets.csv’\n",
            "\n",
            "\rtweets.csv            0%[                    ]       0  --.-KB/s               \rtweets.csv          100%[===================>]  21.01K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-09-11 21:27:28 (28.9 MB/s) - ‘tweets.csv’ saved [21512/21512]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Statistical Classification Models\n",
        "Now, we will conduct stastical-based NLP classification. We are providing a starter code for you, where we train a logistic regression model using  scikit-learn, a widely used NLP library. We use ngram to represent the features and conduct parameter grid search for best parameter selection. Experiment with this code explore other feature representations https://scikit-learn.org/stable/api/sklearn.feature_extraction.html, different preprocessing choices, as well as other classifiers https://scikit-learn.org/stable/modules/linear_model.html to improve classification performance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1xPsSNIR13wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Step 1: Split into train/test\n",
        "X = df['text']\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 2: Vectorize text\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 3: Set up logistic regression and grid search\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l2'],\n",
        "    'solver': ['lbfgs']\n",
        "}\n",
        "\n",
        "lr = LogisticRegression(max_iter=300)\n",
        "grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Step 4: Fit model\n",
        "grid_search.fit(X_train_vec, y_train)\n",
        "\n",
        "# Step 5: Evaluate best model\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.predict(X_test_vec)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qiqbvLjNnSz",
        "outputId": "7c60cf88-6856-4a91-f8cb-b4266b6f245f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Test Accuracy: 0.475\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.82      0.70        17\n",
            "           1       0.25      0.23      0.24        13\n",
            "           2       0.40      0.20      0.27        10\n",
            "\n",
            "    accuracy                           0.47        40\n",
            "   macro avg       0.42      0.42      0.40        40\n",
            "weighted avg       0.44      0.47      0.44        40\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you are familiar with the process, replace \"tweets.csv with your own data and select and run two classification algorithms on your data.\n",
        "\n",
        "\n",
        "Now you have run a classifier on your data! Answer the following questions below\n",
        "\n",
        "1. Which statistical model performed better? Why?\n",
        "\n",
        "\n",
        "2. What preprocessing choices did you make, if any? Did they affect the performance in any way?\n",
        "\n",
        "---\n",
        "\n",
        "ADD YOUR RESPONSES HERE\n",
        "\n"
      ],
      "metadata": {
        "id": "TM_3gtfrmNXD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N25dvF4jvYoy"
      },
      "source": [
        "## Using a pretrained model\n",
        "Now we'll move onto fine-tuning pretrained language models specifically on your dataset. This part of the homework is meant to be an introduction to the HuggingFace library, and it contains code that will potentially be useful for your final projects. Since we're dealing with large models, the first step is to change to a GPU runtime.\n",
        "\n",
        "### Adding a hardware accelerator\n",
        "\n",
        "Please go to the menu and add a GPU as follows:\n",
        "\n",
        "`Edit > Notebook Settings > Hardware accelerator > (GPU)`\n",
        "\n",
        "Run the following cell to confirm that the GPU is detected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edOh9ooiIW1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed54031d-9b5c-4059-bdde-158636e8a656"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Confirm that the GPU is detected\n",
        "\n",
        "assert torch.cuda.is_available()\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = torch.cuda.get_device_name()\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found device: Tesla T4, n_gpu: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrvH7xx9LnMC"
      },
      "source": [
        "## Installing Hugging Face's Transformers library\n",
        "We will use Hugging Face's Transformers (https://github.com/huggingface/transformers), an open-source library that provides general-purpose architectures for natural language understanding and generation with a collection of various pretrained models made by the NLP community. This library will allow us to easily use pretrained models like `BERT` and perform experiments on top of them. We can use these models to solve downstream target tasks, such as text classification, question answering, and sequence labeling.\n",
        "\n",
        "Run the following cell to install Hugging Face's Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtqS2e5fxpqa"
      },
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Taseb33Sovg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b7a71e-a422-47ff-dc1e-99497a90dd0f"
      },
      "source": [
        "# Download helper functions and data\n",
        "!wget https://raw.githubusercontent.com/vperezr/CS7389J_assigments/main/hw1/helpers.py\n",
        "#Import helpers and accuracy functions\n",
        "from helpers import tokenize_and_format, flat_accuracy"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-11 21:29:43--  https://raw.githubusercontent.com/vperezr/CS7389J_assigments/main/hw1/helpers.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1985 (1.9K) [text/plain]\n",
            "Saving to: ‘helpers.py’\n",
            "\n",
            "\rhelpers.py            0%[                    ]       0  --.-KB/s               \rhelpers.py          100%[===================>]   1.94K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-09-11 21:29:44 (38.3 MB/s) - ‘helpers.py’ saved [1985/1985]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx8x6T9-RlhH"
      },
      "source": [
        "##Data Prep and Model Specifications\n",
        "\n",
        "Upload your data using the file explorer to the left. We have provided a function below to tokenize and format your data as BERT requires. Make sure that your csv file, titled final_data.csv, has one column \"text\" and another column \"labels\" containing integers.\n",
        "\n",
        "If you run the cell below without modifications, it will run on the tweets.csv example data we have provided. It imports some helper functions we wrote to demonstrate the task on the sample tweet dataset. You should first run all of the following cells with tweets.csv just to see how everything works. Then, once you understand the whole preprocessing / fine-tuning process, change the csv in the below cell to your final_data.csv file, add any extra preprocessing code you wish, and then run the cells again on your own data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helpers import tokenize_and_format, flat_accuracy\n",
        "import pandas as pd\n",
        "\n",
        "#df = pd.read_csv('final_data.csv')\n",
        "df = pd.read_csv('tweets.csv')\n",
        "\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "texts = df.text.values\n",
        "labels = df.label.values\n",
        "\n",
        "### tokenize_and_format() is a helper function provided in helpers.py ###\n",
        "input_ids, attention_masks = tokenize_and_format(texts)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', texts[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVAfCGO_pIKT",
        "outputId": "7566d0e9-2cb8-47ee-e014-6ed1873ef21d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  @VirginAmerica has getaway deals through May, from $59 one-way. Lots of cool cities http://t.co/tZZJhuIbCH #CheapFlights #FareCompare\n",
            "Token IDs: tensor([  101,  1030,  6261, 14074, 14735,  2038,  2131,  9497,  9144,  2083,\n",
            "         2089,  1010,  2013,  1002,  5354,  2028,  1011,  2126,  1012,  7167,\n",
            "         1997,  4658,  3655,  8299,  1024,  1013,  1013,  1056,  1012,  2522,\n",
            "         1013,  1056, 13213,  3501, 20552,  9818,  2232,  1001, 10036, 28968,\n",
            "         2015,  1001, 13258,  9006, 19362,  2063,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLrFo7XIRvDG"
      },
      "source": [
        "## Create train/test/validation splits\n",
        "\n",
        "Here we split your dataset into 3 parts: a training set, a validation set, and a testing set. Each item in your dataset will be a 3-tuple containing an input_id tensor, an attention_mask tensor, and a label tensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5dOqO3sRyan"
      },
      "source": [
        "total = len(df)\n",
        "\n",
        "num_train = int(total * .8)\n",
        "num_val = int(total * .1)\n",
        "num_test = total - num_train - num_val\n",
        "\n",
        "# make lists of 3-tuples (already shuffled the dataframe in cell above)\n",
        "\n",
        "train_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train)]\n",
        "val_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train, num_val+num_train)]\n",
        "test_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_val + num_train, total)]\n",
        "\n",
        "train_text = [texts[i] for i in range(num_train)]\n",
        "val_text = [texts[i] for i in range(num_train, num_val+num_train)]\n",
        "test_text = [texts[i] for i in range(num_val + num_train, total)]\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCr006iTkqwM"
      },
      "source": [
        "Here we choose the model we want to finetune from https://huggingface.co/transformers/pretrained_models.html. Because the task requires us to label sentences, we wil be using BertForSequenceClassification below. You may see a warning that states that `some weights of the model checkpoint at [model name] were not used when initializing. . .` This warning is expected and means that you should fine-tune your pre-trained model before using it on your downstream task. See [here](https://github.com/huggingface/transformers/issues/5421#issuecomment-652582854) for more info."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPo640_ZlEPK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 908,
          "referenced_widgets": [
            "e5d934fabac347239909c94b7e9ab80d",
            "c80a3e7f234349f59a6eb6d01ffd87d1",
            "36cbd9113b814b6a97dde6c38138f513",
            "7ccf22b67fd64d9490e7947f8a2cf1f8",
            "6cb7e5a14b8e42d9a36465c58fbe068b",
            "7c4d31961c964efdb118548bd7a9bac3",
            "50389c471d0f4ec583e6afae496c6536",
            "bb7b003002c740f1809308ce3eeff41d",
            "6c7f6d4ffea14f2e98dd349564f7893c",
            "cfa98f04627d404cbcbe6634e2c74bac",
            "d374d8e22b9b4ef7b39970451d804520"
          ]
        },
        "outputId": "9660573f-eea3-463b-b343-c76c83149d7e"
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertConfig\n",
        "from torch.optim import AdamW\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 3, # The number of output labels.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5d934fabac347239909c94b7e9ab80d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3lLdoW_le3M"
      },
      "source": [
        "# ACTION REQUIRED #\n",
        "\n",
        "Define your fine-tuning hyperparameters in the cell below (we have randomly picked some values to start with). We want you to experiment with different configurations to find the one that works best (i.e., highest accuracy) on your validation set. Feel free to also change pretrained models to others available in the HuggingFace library (you'll have to modify the cell above to do this). You might find papers on BERT fine-tuning stability (e.g., [Mosbach et al., ICLR 2021](https://openreview.net/pdf?id=nzpLWnVAyah)) to be of interest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd2JdC6IletV"
      },
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "batch_size = 99\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # args.learning_rate - default is 5e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
        "                )\n",
        "epochs = 5"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd4fwn_el1ge"
      },
      "source": [
        "# Fine-tune your model\n",
        "Here we provide code for fine-tuning your model, monitoring the loss, and checking your validation accuracy. Rerun both of the below cells when you change your hyperparameters above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Mzr-kd5RaY"
      },
      "source": [
        "import numpy as np\n",
        "# function to get validation accuracy\n",
        "def get_validation_performance(val_set):\n",
        "    # Put the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    num_batches = int(len(val_set)/batch_size) + 1\n",
        "\n",
        "    total_correct = 0\n",
        "\n",
        "    for i in range(num_batches):\n",
        "\n",
        "      end_index = min(batch_size * (i+1), len(val_set))\n",
        "\n",
        "      batch = val_set[i*batch_size:end_index]\n",
        "\n",
        "      if len(batch) == 0: continue\n",
        "\n",
        "      input_id_tensors = torch.stack([data[0] for data in batch])\n",
        "      input_mask_tensors = torch.stack([data[1] for data in batch])\n",
        "      label_tensors = torch.stack([data[2] for data in batch])\n",
        "\n",
        "      # Move tensors to the GPU\n",
        "      b_input_ids = input_id_tensors.to(device)\n",
        "      b_input_mask = input_mask_tensors.to(device)\n",
        "      b_labels = label_tensors.to(device)\n",
        "\n",
        "      # Tell pytorch not to bother with constructing the compute graph during\n",
        "      # the forward pass, since this is only needed for backprop (training).\n",
        "      with torch.no_grad():\n",
        "\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        outputs = model(b_input_ids,\n",
        "                                token_type_ids=None,\n",
        "                                attention_mask=b_input_mask,\n",
        "                                labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the number of correctly labeled examples in batch\n",
        "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "        labels_flat = label_ids.flatten()\n",
        "        num_correct = np.sum(pred_flat == labels_flat)\n",
        "        total_correct += num_correct\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_correct / len(val_set)\n",
        "    return avg_val_accuracy\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTf_ipbjWNoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02e1dafb-dd34-4ed3-d7fb-2f1eb4e2f5ac"
      },
      "source": [
        "import random\n",
        "\n",
        "# training loop\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    num_batches = int(len(train_set)/batch_size) + 1\n",
        "\n",
        "    for i in range(num_batches):\n",
        "      end_index = min(batch_size * (i+1), len(train_set))\n",
        "\n",
        "      batch = train_set[i*batch_size:end_index]\n",
        "\n",
        "      if len(batch) == 0: continue\n",
        "\n",
        "      input_id_tensors = torch.stack([data[0] for data in batch])\n",
        "      input_mask_tensors = torch.stack([data[1] for data in batch])\n",
        "      label_tensors = torch.stack([data[2] for data in batch])\n",
        "\n",
        "      # Move tensors to the GPU\n",
        "      b_input_ids = input_id_tensors.to(device)\n",
        "      b_input_mask = input_mask_tensors.to(device)\n",
        "      b_labels = label_tensors.to(device)\n",
        "\n",
        "      # Clear the previously calculated gradient\n",
        "      model.zero_grad()\n",
        "\n",
        "      # Perform a forward pass (evaluate the model on this training batch).\n",
        "      outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "      loss = outputs.loss\n",
        "      logits = outputs.logits\n",
        "\n",
        "      total_train_loss += loss.item()\n",
        "\n",
        "      # Perform a backward pass to calculate the gradients.\n",
        "      loss.backward()\n",
        "\n",
        "      # Update parameters and take a step using the computed gradient.\n",
        "      optimizer.step()\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set. Implement this function in the cell above.\n",
        "    print(f\"Total loss: {total_train_loss}\")\n",
        "    val_acc = get_validation_performance(val_set)\n",
        "    print(f\"Validation accuracy: {val_acc}\")\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 5 ========\n",
            "Training...\n",
            "Total loss: 2.2459300756454468\n",
            "Validation accuracy: 0.42105263157894735\n",
            "\n",
            "======== Epoch 2 / 5 ========\n",
            "Training...\n",
            "Total loss: 2.211819291114807\n",
            "Validation accuracy: 0.5263157894736842\n",
            "\n",
            "======== Epoch 3 / 5 ========\n",
            "Training...\n",
            "Total loss: 2.0500255823135376\n",
            "Validation accuracy: 0.42105263157894735\n",
            "\n",
            "======== Epoch 4 / 5 ========\n",
            "Training...\n",
            "Total loss: 2.0188817381858826\n",
            "Validation accuracy: 0.5789473684210527\n",
            "\n",
            "======== Epoch 5 / 5 ========\n",
            "Training...\n",
            "Total loss: 1.8053399324417114\n",
            "Validation accuracy: 0.5263157894736842\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9DpRJE5mHkO"
      },
      "source": [
        "# Evaluate your model on the test set\n",
        "After you're satisfied with your hyperparameters (i.e., you're unable to achieve higher validation accuracy by modifying them further), it's time to evaluate your model on the test set! Run the below cell to compute test set accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msvZ78ii3cZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcdcb722-2428-4365-cc4d-ac538f72fceb"
      },
      "source": [
        "get_validation_performance(test_set)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.6666666666666666)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcMT5aih8xEb"
      },
      "source": [
        "## Question 2.1 (10 points):\n",
        "Congratulations! You've now gone through the entire fine-tuning process and evaluated two type of models for your downstream task. Two more questions. First, describe your hyperparameter selection process when using a transformer model. If you based your process on any research papers or websites, please reference them. Why do you think the hyperparameters you ended up choosing worked better than others? Also, is there a significant discrepancy between your test and validation accuracy? Why do you think this is the case?\n",
        "\n",
        "### *WRITE YOUR ANSWER HERE*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBbdMwt79fIs"
      },
      "source": [
        "## Question 2.2 (20 points):\n",
        "Finally, perform an *error analysis* on your models. This is good practice for your final project. Write some code in the below code cell to print out the text of up to five test set examples that model gets **wrong** for both, your best statistical-based model and the transformer-based model.\n",
        "\n",
        "If the model gets more than five test examples wrong, randomly choose five of them to analyze. If the model gets fewer than five examples wrong, please design five test examples that fool your model (i.e., *adversarial examples*). Then, in the following text cell, perform a qualitative analysis of these examples. See if you can figure out any reasons for errors that you observe, or if you have any informed guesses (e.g., common linguistic properties of these particular examples). Does this analysis suggest any possible future steps to improve your classifier?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X72mumhI9WdR"
      },
      "source": [
        "## YOUR ERROR ANALYSIS CODE HERE\n",
        "## print out up to 5 test set examples (or adversarial examples) that the evaluated model get wrong"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XyBdAup-e6Z"
      },
      "source": [
        "### *DESCRIBE YOUR QUALITATIVE ANALYSIS OF THE ABOVE EXAMPLES HERE*\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szIkBDiQ_Mkv"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Finished? Remember to upload the PDF file, along with the annotation files,  to your canvas assigment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AI Disclosure\n",
        "\n",
        "*   Did you use any AI assistance to complete this homework? If so, please also specify what AI you used.\n",
        "    * *your response here*\n",
        "\n",
        "\n",
        "---\n",
        "*(only complete the below questions if you answered yes above)*\n",
        "\n",
        "*   If you used a large language model to assist you, please paste *all* of the prompts that you used below. Add a separate bullet for each prompt, and specify which problem is associated with which prompt.\n",
        "    * *your response here*\n",
        "*   **Free response**: For each problem for which you used assistance, describe your overall experience with the AI. How helpful was it? Did it just directly give you a good answer, or did you have to edit it? Was its output ever obviously wrong or irrelevant? Did you use it to get the answer or check your own answer?\n",
        "    * *your response here*\n"
      ],
      "metadata": {
        "id": "w6L1RySHhf7v"
      }
    }
  ]
}